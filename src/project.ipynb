{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f97216",
   "metadata": {},
   "source": [
    "# **Uncertainty Project -- Deep Learning**\n",
    "\n",
    "---\n",
    "\n",
    "_Fabio TOCCO, Antoine GUIDON, Yelman YAHI, Anis OUEDGHIRI, Ram NADER_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c70155",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a62d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Literal\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39b2e2",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project directories\n",
    "DATA_ROOT = os.path.join(os.path.pardir, \"data\")\n",
    "MODELS_ROOT = os.path.join(os.path.pardir, \"models\")\n",
    "\n",
    "# Create the directories if they do not exist\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(MODELS_ROOT, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Selected device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a933d3d",
   "metadata": {},
   "source": [
    "## Hyperparameters (DO NOT CHANGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d655717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "EPOCHS: int = 10\n",
    "LEARNING_RATE: float = 1e-4\n",
    "WEIGHT_DECAY: float = 1e-4\n",
    "CRITERION: nn.Module = nn.CrossEntropyLoss()\n",
    "BATCH_SIZE: int = 256\n",
    "\n",
    "# Performance\n",
    "NUM_WORKERS: int = (os.cpu_count() or 0) // 2\n",
    "print(f\"NUM_WORKERS: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed8852",
   "metadata": {},
   "source": [
    "## Parameters (change for different training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cafa38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE_VALUE: int = 32  # resize dataset images to RESIZE_VALUE x RESIZE_VALUE\n",
    "\n",
    "FORCE_RETRAIN: bool = False\n",
    "\n",
    "NORMALIZATION: Literal[\"MNIST\", \"ImageNet\"] = \"MNIST\"\n",
    "SEED: int = 0\n",
    "SHUFFLE: bool = False\n",
    "\n",
    "tools.seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cc3fa",
   "metadata": {},
   "source": [
    "## Datasets & DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8837ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = tools.get_data_transforms(\n",
    "    data_root=DATA_ROOT, resize_value=RESIZE_VALUE, normalization=NORMALIZATION\n",
    ")\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    DATA_ROOT,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=data_transforms,\n",
    ")\n",
    "print(f\"Number of train samples: {len(train_data)}\")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    DATA_ROOT,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=data_transforms,\n",
    ")\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "\n",
    "NUM_CLASSES: int = len(train_data.classes)\n",
    "\n",
    "# Define the validation set by splitting the training data into 2 subsets (80% training and 20% validation)\n",
    "n_train_samples = int(len(train_data) * 0.8)\n",
    "n_validation_samples = len(train_data) - n_train_samples\n",
    "train_data, validation_data = random_split(\n",
    "    train_data, [n_train_samples, n_validation_samples]\n",
    ")\n",
    "\n",
    "train_loader, validation_loader, test_loader = tools.get_loaders(\n",
    "    train_data,\n",
    "    validation_data,\n",
    "    test_data,\n",
    "    shuffle=SHUFFLE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec4db1",
   "metadata": {},
   "source": [
    "## Random MNIST samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SAMPLES, RANDOM_SELECTED_INDEXES = tools.get_random_samples(\n",
    "    test_data, set_size=len(test_data), seed=SEED, num_samples=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcafcf",
   "metadata": {},
   "source": [
    "## Experience #1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ac8a6",
   "metadata": {},
   "source": [
    "### Version 1 - Random weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED: bool = False\n",
    "\n",
    "model_name = tools.get_model_name(\n",
    "    pretrained=PRETRAINED, seed=SEED, normalization=NORMALIZATION\n",
    ")\n",
    "model_dir = os.path.join(MODELS_ROOT, model_name)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have the files of the model, you can only load it or load the metrics to plot them in other cells\n",
    "\n",
    "v1_model = tools.make_resnet18(NUM_CLASSES, pretrained=PRETRAINED)\n",
    "\n",
    "OPTIMIZER = torch.optim.Adam(\n",
    "    v1_model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "v1_model, _, _, _, _ = tools.train_model(\n",
    "    model=v1_model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    criterion=CRITERION,\n",
    "    optimizer=OPTIMIZER,\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    file_path=os.path.join(model_dir, model_name + \".pt\"),\n",
    "    verbose=True,\n",
    "    save_plots=True,\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = tools.evaluate(\n",
    "    v1_model, test_loader, criterion=CRITERION, device=DEVICE\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{model_name} -- Loss on test set: {test_loss:.4f} | Accuracy on test set: {100 * test_accuracy:.2f}%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efab143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = tools.load_metrics(model_dir)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    df_metrics[\"epoch\"],\n",
    "    df_metrics[\"train_acc\"] * 100,\n",
    "    label=\"Train Accuracy\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.plot(\n",
    "    df_metrics[\"epoch\"],\n",
    "    df_metrics[\"val_acc\"] * 100,\n",
    "    label=\"Validation Accuracy\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Version #1 - Evolution of accuracy over epochs\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.xticks(df_metrics[\"epoch\"])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    df_metrics[\"epoch\"],\n",
    "    df_metrics[\"train_loss\"],\n",
    "    label=\"Train Loss\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.plot(\n",
    "    df_metrics[\"epoch\"],\n",
    "    df_metrics[\"val_loss\"],\n",
    "    label=\"Validation Loss\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Version #1 - Evolution of loss over epochs\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xticks(df_metrics[\"epoch\"])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d2545",
   "metadata": {},
   "source": [
    "### Version 2 - Pre-trained weights on ImageNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e146a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED: bool = True\n",
    "\n",
    "model_name = tools.get_model_name(\n",
    "    pretrained=PRETRAINED, seed=SEED, normalization=NORMALIZATION\n",
    ")\n",
    "model_dir = os.path.join(MODELS_ROOT, model_name)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have the files of the model, you can only load it or load the metrics to plot them in other cells\n",
    "\n",
    "v2_model = tools.make_resnet18(NUM_CLASSES, pretrained=PRETRAINED)\n",
    "\n",
    "OPTIMIZER = torch.optim.Adam(\n",
    "    v2_model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "v2_model, _, _, _, _ = tools.train_model(\n",
    "    model=v2_model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    criterion=CRITERION,\n",
    "    optimizer=OPTIMIZER,\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    file_path=os.path.join(model_dir, model_name + \".pt\"),\n",
    "    verbose=True,\n",
    "    save_plots=True,\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = tools.evaluate(\n",
    "    v2_model, test_loader, criterion=CRITERION, device=DEVICE\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{model_name} -- Loss on test set: {test_loss:.4f} | Accuracy on test set: {100 * test_accuracy:.2f}%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = tools.load_metrics(model_dir)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    df_metrics[\"epoch\"],\n",
    "    df_metrics[\"train_acc\"] * 100,\n",
    "    label=\"Train Accuracy\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.plot(\n",
    "    df_metrics[\"epoch\"],\n",
    "    df_metrics[\"val_acc\"] * 100,\n",
    "    label=\"Validation Accuracy\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Version #1 - Evolution of accuracy over epochs\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.xticks(df_metrics[\"epoch\"])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    df_metrics[\"epoch\"],\n",
    "    df_metrics[\"train_loss\"],\n",
    "    label=\"Train Loss\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.plot(\n",
    "    df_metrics[\"epoch\"],\n",
    "    df_metrics[\"val_loss\"],\n",
    "    label=\"Validation Loss\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.title(\"Version #1 - Evolution of loss over epochs\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xticks(df_metrics[\"epoch\"])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a190feb",
   "metadata": {},
   "source": [
    "## Experience #2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8514eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = tools.visualize_predictions(\n",
    "    model=v2_model,\n",
    "    samples=RANDOM_SAMPLES,\n",
    "    device=DEVICE,\n",
    "    title=\"Model Version 2 - Pretrained Weights (ImageNet)\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract images and labels from RANDOM_SAMPLES\n",
    "imgs = torch.stack(\n",
    "    [img for img, _ in RANDOM_SAMPLES]\n",
    ")  # Shape: (20, 3, RESIZE_SIZE, RESIZE_SIZE)\n",
    "labels = [label for _, label in RANDOM_SAMPLES]\n",
    "\n",
    "\n",
    "NOISE_LEVELS: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Apply different levels of Gaussian noise\n",
    "for level in NOISE_LEVELS:\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Gaussian Noise Level: {level}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    if level == 0.0:\n",
    "        noisy_imgs = imgs\n",
    "    else:\n",
    "        noisy_imgs = tools.add_pixel_noise(\n",
    "            imgs,\n",
    "            level=level,\n",
    "            gray=True,  # Bruit en niveaux de gris\n",
    "        )\n",
    "\n",
    "    # Recréer les samples\n",
    "    noisy_samples = [(img, label) for img, label in zip(noisy_imgs, labels)]\n",
    "\n",
    "    # Visualiser\n",
    "    fig = tools.visualize_predictions(\n",
    "        v2_model,\n",
    "        noisy_samples,\n",
    "        device=DEVICE,\n",
    "        figsize=(20, 8),\n",
    "        title=f\"Bruit Gaussien (level={level})\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a202c03",
   "metadata": {},
   "source": [
    "## Experience #3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19706fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# EXPÉRIENCE 3 — Bruit blanc sur MNIST (Single best vs Ensemble)\n",
    "#\n",
    "# Pré-requis AVANT d’exécuter cette cellule :\n",
    "#   - import tools\n",
    "#   - variables globales définies : DATA_ROOT, RESIZE_VALUE, NORMALIZATION, SEED, DEVICE\n",
    "#   - models : liste de 7 chemins .pt (les 7 modèles entraînés sur MNIST)\n",
    "#   - tools.seed_everything(SEED) a déjà été appelé dans le notebook\n",
    "#\n",
    "# Ce que fait cette cellule :\n",
    "#   1) Charge tes 7 modèles et sélectionne le meilleur sur MNIST test complet\n",
    "#   2) Sélectionne 20 images du test MNIST (reproductible)\n",
    "#   3) Applique du bruit blanc gaussien de niveaux : [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "#   4) Pour chaque niveau : crée une grille (20 images) avec S:pred_single(conf) et E:pred_ensemble(conf)\n",
    "#      -> Fichiers sauvegardés : exp3/noise_grid_level_0.0.png ... 0.5.png\n",
    "#   5) Trace l’accuracy en fonction du bruit (single vs ensemble)\n",
    "#      -> Fichier : exp3/noise_accuracy_curve.png\n",
    "# ================================================================\n",
    "\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tools\n",
    "\n",
    "models, model_paths = tools.load_or_train_ensemble(\n",
    "    num_models=7,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=CRITERION,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    "    models_root=MODELS_ROOT,\n",
    "    pretrained=PRETRAINED,\n",
    "    shuffle=SHUFFLE,\n",
    "    normalization=NORMALIZATION,\n",
    "    force_retrain=FORCE_RETRAIN,  # True to force retraining\n",
    "    partial_load=True,  # True to load existing models\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# --------- Config sortie ---------\n",
    "EXP3_DIR = Path(\"exp3\")\n",
    "EXP3_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --------- Vérifs ---------\n",
    "try:\n",
    "    DEVICE\n",
    "except NameError:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert \"models\" in globals() and len(models) == 7, (\n",
    "    \"Il faut définir `models` = liste de 7 chemins .pt\"\n",
    ")\n",
    "\n",
    "# --------- Data & transforms ---------\n",
    "tf = tools.get_data_transforms(DATA_ROOT, RESIZE_VALUE, NORMALIZATION)\n",
    "to_tensor = transforms.ToTensor()  # PIL -> [0,1] tensor\n",
    "to_pil = transforms.ToPILImage()  # tensor -> PIL\n",
    "\n",
    "# Datasets\n",
    "mnist_raw = datasets.MNIST(DATA_ROOT, train=False, download=True, transform=None)\n",
    "mnist_test = datasets.MNIST(DATA_ROOT, train=False, download=True, transform=tf)\n",
    "\n",
    "# Loader pour évaluer le meilleur modèle sur le test complet\n",
    "test_loader_mnist = DataLoader(mnist_test, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "# Sélection de 20 indices (reproductible)\n",
    "rng = random.Random(SEED)\n",
    "mnist_idx = sorted(rng.sample(range(len(mnist_raw)), 20))\n",
    "print(f\"[Exp3] Indices MNIST (20) : {mnist_idx}\")\n",
    "\n",
    "# --------- Chargement des 7 modèles ---------\n",
    "nets = []\n",
    "for p in model_paths:\n",
    "    net = tools.make_resnet18(num_classes=10, pretrained=False)\n",
    "    net = tools.load_model(net, file_path=p, device=DEVICE).eval()\n",
    "    nets.append(net)\n",
    "print(f\"[Exp3] {len(nets)} modèles chargés.\")\n",
    "\n",
    "# --------- Sélection du meilleur modèle (accuracy sur test MNIST complet) ---------\n",
    "crit = nn.CrossEntropyLoss()\n",
    "accs = []\n",
    "for i, m in enumerate(nets, 1):\n",
    "    loss, acc = tools.evaluate(m, test_loader_mnist, criterion=crit, device=DEVICE)\n",
    "    accs.append(acc)\n",
    "    print(f\"Model {i}: Loss={loss:.4f}, Acc={acc * 100:.2f}%\")\n",
    "best_i = int(np.argmax(accs))\n",
    "best_model = nets[best_i]\n",
    "print(f\"[Exp3] Meilleur modèle : model_{best_i + 1} (Acc={accs[best_i] * 100:.2f}%)\")\n",
    "\n",
    "\n",
    "# --------- Helpers ---------\n",
    "@torch.no_grad()\n",
    "def softmax_single(model, x):\n",
    "    return torch.softmax(model(x.to(DEVICE)), dim=1)  # (B,10)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def softmax_ensemble(model_list, x):\n",
    "    s = None\n",
    "    for m in model_list:\n",
    "        p = softmax_single(m, x)\n",
    "        s = p if s is None else (s + p)\n",
    "    return s / len(model_list)\n",
    "\n",
    "\n",
    "def add_gaussian_noise_pil(pil_img, sigma):\n",
    "    \"\"\"Ajoute un bruit gaussien N(0, sigma) canal-par-canal, clampé sur [0,1], puis retourne une PIL.\"\"\"\n",
    "    t = to_tensor(pil_img)  # [0,1], (C,H,W)\n",
    "    if sigma > 0:\n",
    "        noise = torch.randn_like(t) * sigma\n",
    "        t = (t + noise).clamp(0.0, 1.0)\n",
    "    return to_pil(t)\n",
    "\n",
    "\n",
    "def build_noisy_batch_from_indices(raw_ds, idx_list, sigma):\n",
    "    \"\"\"Construit un batch (N,C,H,W) transformé (tf) après ajout de bruit sur PIL, + liste des PIL bruitées et labels.\"\"\"\n",
    "    pil_list, labels = [], []\n",
    "    for idx in idx_list:\n",
    "        pil, y = raw_ds[idx]\n",
    "        pil_n = add_gaussian_noise_pil(pil, sigma)\n",
    "        pil_list.append(pil_n)\n",
    "        labels.append(int(y))\n",
    "    x = torch.stack([tf(p) for p in pil_list], dim=0)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    return pil_list, x, y\n",
    "\n",
    "\n",
    "def save_grid(pil_imgs, y_true, pred_s, conf_s, pred_e, conf_e, title, out_path):\n",
    "    n = len(pil_imgs)\n",
    "    cols = 10\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(1.6 * cols, 2.2 * rows))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(n):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(pil_imgs[i], cmap=\"gray\")\n",
    "        ax.set_title(\n",
    "            f\"y={y_true[i]}\\nS:{pred_s[i]}({conf_s[i]:.2f})  E:{pred_e[i]}({conf_e[i]:.2f})\",\n",
    "            fontsize=7,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    plt.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --------- Expérience : niveaux de bruit ---------\n",
    "NOISE_LEVELS = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "acc_single, acc_ens = [], []\n",
    "\n",
    "for sigma in NOISE_LEVELS:\n",
    "    # Préparer batch bruité\n",
    "    pil_list, x, y = build_noisy_batch_from_indices(mnist_raw, mnist_idx, sigma)\n",
    "\n",
    "    # Prédictions\n",
    "    p_single = softmax_single(best_model, x)\n",
    "    p_ensemble = softmax_ensemble(nets, x)\n",
    "\n",
    "    pred_s = p_single.argmax(1).cpu().numpy()\n",
    "    pred_e = p_ensemble.argmax(1).cpu().numpy()\n",
    "\n",
    "    conf_s = p_single.max(1).values.cpu().numpy()\n",
    "    conf_e = p_ensemble.max(1).values.cpu().numpy()\n",
    "\n",
    "    # Accuracy\n",
    "    a_s = (pred_s == y.numpy()).mean()\n",
    "    a_e = (pred_e == y.numpy()).mean()\n",
    "    acc_single.append(a_s)\n",
    "    acc_ens.append(a_e)\n",
    "\n",
    "    # Grilles à sauvegarder\n",
    "    out_name = EXP3_DIR / f\"noise_grid_level_{sigma:.1f}.png\"\n",
    "    save_grid(\n",
    "        pil_list,\n",
    "        y.numpy(),\n",
    "        pred_s,\n",
    "        conf_s,\n",
    "        pred_e,\n",
    "        conf_e,\n",
    "        title=f\"Exp3 — MNIST (20 images) • Bruit σ={sigma:.1f}\",\n",
    "        out_path=out_name,\n",
    "    )\n",
    "    print(\n",
    "        f\"[Exp3] Grid sauvegardée → {out_name} | Acc Single={a_s * 100:.1f}%  Ens={a_e * 100:.1f}%\"\n",
    "    )\n",
    "\n",
    "# --------- Courbe Accuracy vs Niveau de bruit ---------\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(\n",
    "    NOISE_LEVELS,\n",
    "    np.array(acc_single) * 100,\n",
    "    marker=\"o\",\n",
    "    label=\"Single (meilleur modèle)\",\n",
    ")\n",
    "plt.plot(\n",
    "    NOISE_LEVELS, np.array(acc_ens) * 100, marker=\"o\", label=\"Ensemble (moy. softmax)\"\n",
    ")\n",
    "plt.xlabel(\"Niveau de bruit σ\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Exp3 — Accuracy vs. niveau de bruit (MNIST, 20 images)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "acc_curve_path = EXP3_DIR / \"noise_accuracy_curve.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(acc_curve_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"[Exp3] Courbe sauvegardée → {acc_curve_path}\")\n",
    "\n",
    "print(\"\\n[Exp3] Terminé. Fichiers générés dans le dossier: exp3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c31de",
   "metadata": {},
   "source": [
    "## Experience #4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# EXPÉRIENCE 4 – MNIST vs KMNIST (Single Model vs Ensemble)\n",
    "#\n",
    "# Pour que cette cellule fonctionne, il faut AVANT :\n",
    "#    1. Avoir importé ton module `tools` (qui contient load_model, make_resnet18, get_data_transforms, evaluate, seed_everything, etc.)\n",
    "#    2. Avoir défini dans le notebook :\n",
    "#         - DATA_ROOT : chemin du dataset MNIST/KMNIST\n",
    "#         - RESIZE_VALUE : taille d’entrée (ex: 32 ou 224)\n",
    "#         - NORMALIZATION : \"MNIST\" ou \"ImageNet\"\n",
    "#         - SEED : entier pour reproductibilité\n",
    "#         - DEVICE : torch.device(\"cuda\") ou torch.device(\"cpu\")\n",
    "#         - models : liste contenant les 7 chemins complets des poids (.pt)\n",
    "#\n",
    "# Exemple minimal AVANT cette cellule :\n",
    "#   import tools\n",
    "#   DATA_ROOT = \"./data\"\n",
    "#   RESIZE_VALUE = 32\n",
    "#   NORMALIZATION = \"MNIST\"\n",
    "#   SEED = 0\n",
    "#   DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#   models = [\n",
    "#       r\"C:\\...\\resnet18_normMNIST_no-shuffle_seed0_1\\resnet18_normMNIST_no-shuffle_seed0_1.pt\",\n",
    "#       ...\n",
    "#   ]  # 7 fichiers .pt\n",
    "#   tools.seed_everything(SEED)\n",
    "#\n",
    "# Cette cellule :\n",
    "#    • Charge les 7 modèles\n",
    "#    • Sélectionne 20 images MNIST et 20 KMNIST\n",
    "#    • Identifie le meilleur modèle (single) sur le test MNIST complet\n",
    "#    • Compare prédictions Single vs Ensemble (mean softmax)\n",
    "#    • Affiche accuracy et sauvegarde 2 grilles d’images (MNIST & KMNIST)\n",
    "#    • Stocke les résultats dans le dossier \"exp4/\"\n",
    "# ================================================================\n",
    "\n",
    "import os, random, csv\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tools\n",
    "\n",
    "models, model_paths = tools.load_or_train_ensemble(\n",
    "    num_models=7,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=CRITERION,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    "    models_root=MODELS_ROOT,\n",
    "    pretrained=PRETRAINED,\n",
    "    shuffle=SHUFFLE,\n",
    "    normalization=NORMALIZATION,\n",
    "    force_retrain=FORCE_RETRAIN,  # True to force retraining\n",
    "    partial_load=True,  # True to load existing models\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# --------- Config & dossier de sortie ---------\n",
    "EXP_DIR = Path(\"exp4\")\n",
    "EXP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    DEVICE\n",
    "except NameError:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert \"models\" in globals() and len(models) == 7, (\n",
    "    \"Il faut définir `models` = liste de 7 chemins .pt\"\n",
    ")\n",
    "\n",
    "# --------- Chargement transforms & datasets ---------\n",
    "tf = tools.get_data_transforms(DATA_ROOT, RESIZE_VALUE, NORMALIZATION)\n",
    "\n",
    "mnist_raw = datasets.MNIST(DATA_ROOT, train=False, download=True, transform=None)\n",
    "kmnist_raw = datasets.KMNIST(DATA_ROOT, train=False, download=True, transform=None)\n",
    "\n",
    "mnist_test = datasets.MNIST(DATA_ROOT, train=False, download=True, transform=tf)\n",
    "\n",
    "test_loader_mnist = DataLoader(mnist_test, batch_size=256, shuffle=False)\n",
    "\n",
    "# --------- Sélection de 20 indices pour MNIST & KMNIST ---------\n",
    "rng = random.Random(SEED)\n",
    "mnist_idx = sorted(rng.sample(range(len(mnist_raw)), 20))\n",
    "kmnist_idx = sorted(rng.sample(range(len(kmnist_raw)), 20))\n",
    "\n",
    "print(f\"[Exp4] Indices MNIST (20)  : {mnist_idx}\")\n",
    "print(f\"[Exp4] Indices KMNIST (20) : {kmnist_idx}\")\n",
    "\n",
    "# --------- Chargement des 7 modèles ---------\n",
    "nets = []\n",
    "for p in model_paths:\n",
    "    net = tools.make_resnet18(num_classes=10, pretrained=False)\n",
    "    net = tools.load_model(net, file_path=p, device=DEVICE).eval()\n",
    "    nets.append(net)\n",
    "\n",
    "print(f\"[Exp4] {len(nets)} modèles chargés.\")\n",
    "\n",
    "# --------- Trouver le meilleur modèle (sur test MNIST) ---------\n",
    "crit = nn.CrossEntropyLoss()\n",
    "accs = []\n",
    "for i, m in enumerate(nets, 1):\n",
    "    loss, acc = tools.evaluate(m, test_loader_mnist, criterion=crit, device=DEVICE)\n",
    "    accs.append(acc)\n",
    "    print(f\"Model {i}: Loss={loss:.4f}, Acc={acc * 100:.2f}%\")\n",
    "\n",
    "best_i = int(np.argmax(accs))\n",
    "best_model = nets[best_i]\n",
    "print(f\"[Exp4] Meilleur modèle : model_{best_i + 1} (Acc={accs[best_i] * 100:.2f}%)\")\n",
    "\n",
    "\n",
    "# --------- Fonctions de prédiction ---------\n",
    "@torch.no_grad()\n",
    "def softmax_single(model, x):\n",
    "    return torch.softmax(model(x.to(DEVICE)), dim=1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def softmax_ensemble(model_list, x):\n",
    "    s = None\n",
    "    for m in model_list:\n",
    "        p = softmax_single(m, x)\n",
    "        s = p if s is None else s + p\n",
    "    return s / len(model_list)\n",
    "\n",
    "\n",
    "def pil_batch(raw_ds, idx_list):\n",
    "    imgs, labels = [], []\n",
    "    for idx in idx_list:\n",
    "        img, y = raw_ds[idx]\n",
    "        imgs.append(img)\n",
    "        labels.append(int(y))\n",
    "    return imgs, torch.tensor(labels)\n",
    "\n",
    "\n",
    "def pil_to_tensor_batch(pil_list):\n",
    "    return torch.stack([tf(p) for p in pil_list], dim=0)\n",
    "\n",
    "\n",
    "# --------- Préparer les 20 images MNIST & KMNIST ---------\n",
    "mnist_pil, mnist_y = pil_batch(mnist_raw, mnist_idx)\n",
    "kmnist_pil, kmnist_y = pil_batch(kmnist_raw, kmnist_idx)\n",
    "\n",
    "mnist_x = pil_to_tensor_batch(mnist_pil)\n",
    "kmnist_x = pil_to_tensor_batch(kmnist_pil)\n",
    "\n",
    "# --------- Prédictions modèle seul et ensemble ---------\n",
    "mnist_p_single = softmax_single(best_model, mnist_x)\n",
    "mnist_p_ensemble = softmax_ensemble(nets, mnist_x)\n",
    "\n",
    "kmnist_p_single = softmax_single(best_model, kmnist_x)\n",
    "kmnist_p_ensemble = softmax_ensemble(nets, kmnist_x)\n",
    "\n",
    "mnist_pred_s = mnist_p_single.argmax(1).cpu()\n",
    "mnist_pred_e = mnist_p_ensemble.argmax(1).cpu()\n",
    "\n",
    "kmnist_pred_s = kmnist_p_single.argmax(1).cpu()\n",
    "kmnist_pred_e = kmnist_p_ensemble.argmax(1).cpu()\n",
    "\n",
    "# --------- Accuracy comparée ---------\n",
    "mnist_acc_s = (mnist_pred_s == mnist_y).float().mean().item()\n",
    "mnist_acc_e = (mnist_pred_e == mnist_y).float().mean().item()\n",
    "kmnist_acc_s = (kmnist_pred_s == kmnist_y).float().mean().item()\n",
    "kmnist_acc_e = (kmnist_pred_e == kmnist_y).float().mean().item()\n",
    "\n",
    "print(\n",
    "    f\"\\n[Exp4] Accuracy sur 20 MNIST  : Single={mnist_acc_s * 100:.2f}% | Ensemble={mnist_acc_e * 100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"[Exp4] Accuracy sur 20 KMNIST : Single={kmnist_acc_s * 100:.2f}% | Ensemble={kmnist_acc_e * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "\n",
    "# --------- Grilles d’images (fonction existante) ---------\n",
    "def save_grid(pil_imgs, y_true, pred_s, conf_s, pred_e, conf_e, title, out_path):\n",
    "    n = len(pil_imgs)\n",
    "    cols = 10\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(1.6 * cols, 2.2 * rows))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(n):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(pil_imgs[i], cmap=\"gray\")\n",
    "        ax.set_title(\n",
    "            f\"y={y_true[i]}\\nS:{pred_s[i]}({conf_s[i]:.2f})  E:{pred_e[i]}({conf_e[i]:.2f})\",\n",
    "            fontsize=7,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    plt.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "mnist_conf_s = mnist_p_single.max(1).values.cpu().numpy()\n",
    "mnist_conf_e = mnist_p_ensemble.max(1).values.cpu().numpy()\n",
    "\n",
    "kmnist_conf_s = kmnist_p_single.max(1).values.cpu().numpy()\n",
    "kmnist_conf_e = kmnist_p_ensemble.max(1).values.cpu().numpy()\n",
    "\n",
    "mnist_grid = EXP_DIR / \"mnist_grid_20.png\"\n",
    "kmnist_grid = EXP_DIR / \"kmnist_grid_20.png\"\n",
    "\n",
    "save_grid(\n",
    "    mnist_pil,\n",
    "    mnist_y.numpy(),\n",
    "    mnist_pred_s.numpy(),\n",
    "    mnist_conf_s,\n",
    "    mnist_pred_e.numpy(),\n",
    "    mnist_conf_e,\n",
    "    \"Exp4 — MNIST (20 images)\",\n",
    "    mnist_grid,\n",
    ")\n",
    "\n",
    "save_grid(\n",
    "    kmnist_pil,\n",
    "    kmnist_y.numpy(),\n",
    "    kmnist_pred_s.numpy(),\n",
    "    kmnist_conf_s,\n",
    "    kmnist_pred_e.numpy(),\n",
    "    kmnist_conf_e,\n",
    "    \"Exp4 — KMNIST (20 images)\",\n",
    "    kmnist_grid,\n",
    ")\n",
    "\n",
    "print(f\"[Exp4] Terminé — images sauvegardées dans: {EXP_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bd88c",
   "metadata": {},
   "source": [
    "## Experience #5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5983a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS: int = 7\n",
    "PRETRAINED: bool = False\n",
    "\n",
    "models, model_paths = tools.load_or_train_ensemble(\n",
    "    num_models=NUM_MODELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=CRITERION,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    "    models_root=MODELS_ROOT,\n",
    "    pretrained=PRETRAINED,\n",
    "    shuffle=SHUFFLE,\n",
    "    normalization=NORMALIZATION,\n",
    "    force_retrain=FORCE_RETRAIN,\n",
    "    partial_load=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nEnsemble prêt avec {len(models)} modèles!\")\n",
    "print(f\"Chemins des modèles:\")\n",
    "for i, path in enumerate(model_paths, 1):\n",
    "    print(f\"  {i}. {path}\")\n",
    "\n",
    "\n",
    "ANGLES: list[int] = list(range(0, 361, 10))  # 0, 10, ..., 360\n",
    "NUM_SAMPLES: int = 20\n",
    "\n",
    "\n",
    "# Extract images and labels from RANDOM_SAMPLES\n",
    "imgs = torch.stack([img for img, _ in RANDOM_SAMPLES])  # Shape: (20, 3, 32, 32)\n",
    "labels = torch.tensor([y for _, y in RANDOM_SAMPLES])  # Shape: (20,)\n",
    "\n",
    "print(f\"Indices sélectionnés: {RANDOM_SELECTED_INDEXES}\")\n",
    "print(f\"Batch shape: {imgs.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Labels: {labels.tolist()}\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. FOLLOW 4 SPECIFIC IMAGES FOR DETAILED ANALYSIS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "track_indices = RANDOM_SELECTED_INDEXES[:4]\n",
    "per_image_data = {\n",
    "    idx: {\n",
    "        \"label\": int(labels[i]),  # True label\n",
    "        \"angle_probs\": {},  # Mean probabilities per angle\n",
    "        \"predictions\": {},  # Prediction per angle\n",
    "    }\n",
    "    for i, idx in enumerate(track_indices)\n",
    "}\n",
    "\n",
    "print(f\"Images suivies pour analyse détaillée: {track_indices}\")\n",
    "print(f\"Labels vrais: {[per_image_data[idx]['label'] for idx in track_indices]}\\n\")\n",
    "\n",
    "\n",
    "print(\"Angle | Accuracy (moyenne softmax sur 7 modèles)\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "accuracies = []  # Stocker les accuracies pour plot\n",
    "\n",
    "for angle in ANGLES:\n",
    "    rot_imgs = TF.rotate(\n",
    "        imgs,\n",
    "        angle,\n",
    "        interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "    )  # Shape: (20, 3, 32, 32)\n",
    "\n",
    "    # Mean predictions over the 7 models\n",
    "    mean_probs = tools.get_mean_probs_fast(\n",
    "        rot_imgs, models, device=DEVICE\n",
    "    )  # Shape: (20, 10)\n",
    "\n",
    "    preds = mean_probs.argmax(dim=1)  # Shape: (20,)\n",
    "\n",
    "    correct = (preds.cpu() == labels).sum().item()\n",
    "    acc = correct / NUM_SAMPLES\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    print(f\"{angle:5d}° | {acc:.2%}\")\n",
    "\n",
    "    for i, idx in enumerate(track_indices):\n",
    "        per_image_data[idx][\"angle_probs\"][angle] = mean_probs[i].cpu().clone()\n",
    "        per_image_data[idx][\"predictions\"][angle] = preds[i].item()\n",
    "\n",
    "print(f\"\\nÉvaluation terminée sur {len(ANGLES)} angles de rotation.\\n\")\n",
    "\n",
    "\n",
    "show_angles = [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 360]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"APERÇU DÉTAILLÉ DES PROBABILITÉS MOYENNES (4 images suivies)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx in track_indices:\n",
    "    true_y = per_image_data[idx][\"label\"]\n",
    "    print(f\"\\nImage idx={idx} | Vrai label: {true_y}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\" Angle | Pred | P(pred) | P(vrai) | Correct?\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for angle in show_angles:\n",
    "        if angle not in per_image_data[idx][\"angle_probs\"]:\n",
    "            continue\n",
    "\n",
    "        probs = per_image_data[idx][\"angle_probs\"][angle]\n",
    "        pred = per_image_data[idx][\"predictions\"][angle]\n",
    "\n",
    "        p_pred = probs[pred].item()\n",
    "        p_true = probs[true_y].item()\n",
    "        is_correct = \"✓\" if pred == true_y else \"✗\"\n",
    "\n",
    "        print(f\"{angle:6d}° | {pred:4d} | {p_pred:7.3f} | {p_true:7.3f} | {is_correct}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ANGLES, accuracies, marker=\"o\", linewidth=2, markersize=6, color=\"steelblue\")\n",
    "plt.axhline(y=1.0, color=\"green\", linestyle=\"--\", alpha=0.3, label=\"Perfect accuracy\")\n",
    "plt.xlabel(\"Angle de rotation (°)\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Robustesse de l'ensemble de modèles aux rotations\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAccuracy min: {min(accuracies):.2%} | Accuracy max: {max(accuracies):.2%}\")\n",
    "print(f\"Accuracy moyenne: {np.mean(accuracies):.2%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "for idx in track_indices:\n",
    "    label = per_image_data[idx][\"label\"]\n",
    "    angles = sorted(per_image_data[idx][\"angle_probs\"].keys())\n",
    "\n",
    "    # Probability of the true class at each angle\n",
    "    probs_true = [per_image_data[idx][\"angle_probs\"][a][label].item() for a in angles]\n",
    "\n",
    "    plt.plot(\n",
    "        angles,\n",
    "        probs_true,\n",
    "        marker=\"o\",\n",
    "        label=f\"Chiffre {label} (idx={idx})\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Angle de rotation (°)\", fontsize=12)\n",
    "plt.ylabel(\"P(vraie classe)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Confiance du modèle sur la vraie classe vs angle de rotation\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.legend(title=\"Vrai label\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.3, label=\"Seuil 50%\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8. HEATMAP OF CLASS PROBABILITIES VS ROTATION ANGLE (4 TRACKED IMAGES)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(track_indices):\n",
    "    ax = axes[i]\n",
    "    label = per_image_data[idx][\"label\"]\n",
    "\n",
    "    # Matrix (angle, classe) of probabilities\n",
    "    angles = sorted(per_image_data[idx][\"angle_probs\"].keys())\n",
    "    prob_matrix = np.array(\n",
    "        [per_image_data[idx][\"angle_probs\"][a].numpy() for a in angles]\n",
    "    )  # Shape: (len(angles), 10)\n",
    "\n",
    "    im = ax.imshow(prob_matrix.T, aspect=\"auto\", cmap=\"viridis\", vmin=0, vmax=1)\n",
    "\n",
    "    ax.set_xlabel(\"Angle (index)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Classe\", fontsize=10)\n",
    "    ax.set_title(f\"Image {idx} | Vrai label: {label}\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_yticks(range(10))\n",
    "    ax.set_xticks(range(0, len(angles), 5))\n",
    "    ax.set_xticklabels([angles[i] for i in range(0, len(angles), 5)])\n",
    "\n",
    "    # Highlight true class\n",
    "    ax.axhline(y=label, color=\"red\", linestyle=\"--\", linewidth=2, alpha=0.5)\n",
    "\n",
    "    plt.colorbar(im, ax=ax, label=\"Probabilité\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Heatmap des probabilités par classe et angle de rotation\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.995,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalyse complète terminée!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
